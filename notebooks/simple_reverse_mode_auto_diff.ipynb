{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Automatic differentialtion\n",
    "- chain-rule\n",
    "- closed-form symbolic derivative\n",
    "- finite differences method\n",
    "- back propagation\n",
    "- primal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp, sin, cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple funciton example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note we are restricting ourselves to a computational chain, i.e. we have an input x which goes through multiple operations which has a 1 to 1 mapping\n",
    "f = lambda x: exp(sin(sin(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2013533791690376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we take the closed-form symbolic derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so going through each element we take the derivative, i.e. the derivative of the exponential is just the exponential, then we go through the layers deeper\n",
    "f_prime = lambda x: exp(sin(sin(x))) * cos(sin(x)) * cos(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.562752038662712"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's query our derivative function at the same point\n",
    "f_prime(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can validate this derivative using finite differences approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5627520671680486"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we take the original primal function and evaluate it at 2.0 with a small value\n",
    "(f(2.0 + 1e-8) - f(2.0)) / 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so now we need to build our primatives which are the exp and sin which was used in f:\n",
    "\n",
    "# note there are some coroutines which can compute the sine and cosine at the same time instead as below which are done seperatly\n",
    "def sin_backprop_rule(x):\n",
    "    \"\"\"back-pop rule returns both the primal output as well as the pullback operation\"\"\"\n",
    "    # first it produces a primal output y\n",
    "    y = sin(x)\n",
    "\n",
    "    # then it defines a pullback operation which is a closure function which we will return which will allow us to back propergate cotangent information on x\n",
    "    def sin_pullback(y_cotangent):\n",
    "        x_cotangent = y_cotangent * cos(x)\n",
    "        return x_cotangent\n",
    "\n",
    "    return y, sin_pullback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_backprop_rule(x):\n",
    "    \"\"\"This takes a primal input x and produces a primal output y\"\"\"\n",
    "    y = exp(x)\n",
    "\n",
    "    def exp_pullback(y_cotangent):\n",
    "        \"\"\"This closure function captures the cotangent information of the output which is the value of y \n",
    "        and back-props to the cotangent of the input of x\n",
    "        \"\"\"\n",
    "        x_cotangent = y_cotangent * y\n",
    "        return x_cotangent\n",
    "    \n",
    "    return y, exp_pullback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (rule library as a dict) - associate a back-prop rule to each of the functions we want to use in our computational chain\n",
    "primative_rules: dict = {\n",
    "    sin: sin_backprop_rule,\n",
    "    exp: exp_backprop_rule\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector-jacobian-product\n",
    "def vjp(chain: list, primal):\n",
    "    \"\"\" A function which takes in a computational chain, as well as a primal point in which you want to evaluate the chain\n",
    "    - it produces a primal pass and records the pullback operations\n",
    "    - then it produces a vector jacobian product function (or a pullback function) which you can query cotangent information\n",
    "    - then you can back propogate that\n",
    "    \"\"\"\n",
    "\n",
    "    # create a container to record the pullback operations\n",
    "    pullback_stack: list = []\n",
    "    \n",
    "    # starting value position in which we want to evaluate the chain at\n",
    "    current_value = primal\n",
    "\n",
    "    # primal pass\n",
    "    for operation in chain:\n",
    "        # retrieve the rule for particular primative operation\n",
    "        rule = primative_rules[operation]\n",
    "        # this will return a value which we will override as well as another function\n",
    "        current_value, current_pullback = rule(current_value)\n",
    "        # so we will be saving a function in this list container\n",
    "        pullback_stack.append(current_pullback)\n",
    "\n",
    "    def pullback(cotangent):\n",
    "        \"\"\"\"\"\"\n",
    "        # reverse pass\n",
    "        current_cotangent = cotangent\n",
    "        for back in reversed(pullback_stack):\n",
    "            current_cotangent = back(current_cotangent)\n",
    "\n",
    "        return current_cotangent\n",
    "\n",
    "    return current_value, pullback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function we want to evaluate would be the chain of operations going from the inner most to the outer i.e. exp(sin(sin(x))) to [sin, sin, exp]\n",
    "out, back = vjp([sin, sin, exp], 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2013533791690376"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the same as the primal operation\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.562752038662712"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we call it at 1.0 to get the derivative because we are evaluation a vjp computes the effect of the jacobian of it was left multiplied with a vector and if the scaler is 1.0 then we are not scaling it but we are just evaluating the derivative as is\n",
    "back(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function\n",
    "def val_and_grad(chain, x):\n",
    "    \"\"\"\"\"\"\n",
    "    y, back = vjp(chain, x)\n",
    "    derivative = back(1.0)\n",
    "    return y, derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.2013533791690376, -0.562752038662712)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_and_grad([sin, sin, exp], 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.2013533791690376, -0.562752038662712)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(2.0), f_prime(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to compare, AD in general provides derivative information at machine precision, so it uses the same algorithmic implementation of the graph of the function that you use for the primal evaluation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
